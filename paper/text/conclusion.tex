\chapter*{Conclusion}\addcontentsline{toc}{chapter}{Conclusion}\markboth{Conclusion}{Conclusion}

In conclusion, the objectives of this thesis have been achieved. The \nameref{part:conceptual-framework} established a theoretical framework and provided an overview of the current state of project adaptation for the code completion task. This chapter supplies the necessary background to assess the practical contributions of the work, which are detailed in \nameref{part:applied-research}. Specifically, four research questions regarding the impact of context composition strategies on code completion quality were addressed, including the examination of three different settings: inference-only, fine-tuning, and context extension. To accomplish this, a context composition framework and a fine-tuning pipeline were implemented.

The conducted research yielded several significant findings regarding repository-level code completion. First, the results substantiate the crucial role of repository context in enhancing completion quality under the inference of a pre-trained Code LLM setup. Second, DeepSeek-Coder-Base 1.3B possesses sufficient training, with its in-context learning capabilities exhibiting only marginal variability following downstream fine-tuning on different context composition strategies. Third, it was demonstrated that in-context learning capabilities are not affected by a properly conducted repository-level pre-training stage and are maintained after the model acquires a comprehension of long-context data. Fourth, the context composition strategy employed during the aforementioned stage demonstrates minimal impact on final model quality, suggesting that RoPE adjustment serves as the primary driver of long-context improvements.

Additional observations were also inferred beyond the originally stated research questions. Most notably, the computational requirements of the repository-level pre-training stage can be substantially reduced while maintaining competitive results not only with the extended context window but also with the original context length and a relatively small number of training tokens.

Overall, this thesis contributes to the evolution of code completion models by underscoring theoretical foundations in the conceptual framework and offering valuable empirical insights for practitioners through comprehensive research on context composition techniques for repository-level understanding.
