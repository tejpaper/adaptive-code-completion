\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}\markboth{Introduction}{Introduction}

Software development plays a pivotal role in shaping the modern digital world. Assisting developers in writing code more efficiently has the potential to accelerate innovation across industries by reducing the cognitive and temporal overhead of programming. Over the decades, numerous tools have emerged to support developers, including high-level programming languages, integrated development environments, version control systems, and, more recently, AI-powered code assistance. Tasks such as code completion, generation, refactoring, and bug localization are increasingly addressed using Large Language Models (LLMs), which have demonstrated strong capabilities in natural language and code understanding.

Code completion, in particular, benefits significantly from the in-context learning capabilities of LLMs---where the model leverages examples or relevant information provided in the input context to improve predictions without parameter updates. However, while LLMs have advanced in modeling local contexts, a critical limitation remains: their restricted ability to integrate and reason over information dispersed across large codebases. This includes understanding dependencies between files, class hierarchies, and interactions with external libraries---information that is often essential for generating coherent and accurate completions.

Despite the emergence of code-specific LLMs and efforts to incorporate repository-level context during training or inference, most pre-trained models still struggle to process more than a small fraction of a project's code files at once. As a result, they fall short in capturing project-wide structure, leading to incomplete or inaccurate predictions. Therefore, there is a need for continued research within the community developing such models to address these limitations. This thesis focuses on exploring how the composition and organization of contextual information influence the performance of code completion models, particularly in repository-level settings where code is distributed across multiple interdependent files. It investigates how context selection strategies and model adaptations can enhance the ability of pre-trained Code Large Language Models (Code LLMs) to generate coherent and accurate completions that reflect the structure and semantics of entire software projects.

The thesis is organized as follows: \chapterref{chap:code-completion} offers an overview of the code completion task, including its definition, significance, historical approaches, and taxonomy. \chapterref{chap:standard-lm} introduces essential concepts related to Language Modeling (LM) and state-of-the-art architecture for this domain within machine learning. In \chapterref{chap:completion-centric-lm}, the discussion on language modeling is extended to encompass code completion with relevant details. \chapterref{chap:in-context-learning} covers the aspects of in-context learning, which is widely adopted in code-related tasks. The implementation of the tools used to answer the research questions is elaborated in \chapterref{chap:technical-foundation}. Finally, \chapterref{chap:research-investigation} describes the research, including precise question formulations, experimental design, and results interpretation.
