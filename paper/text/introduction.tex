\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}\markboth{Introduction}{Introduction}

Software development plays a pivotal role in shaping the modern digital world. Assisting developers in writing code more efficiently has the potential to accelerate innovation across industries by reducing the cognitive and temporal overhead of programming. Over the decades, numerous tools have emerged to support developers, including high-level programming languages, integrated development environments, version control systems, and, more recently, AI-powered code assistance. Tasks such as code completion, generation, refactoring, and bug localization are increasingly addressed using Large Language Models (LLMs), which have demonstrated strong capabilities in natural language and code understanding.

Code completion, in particular, benefits significantly from the in-context learning capabilities of LLMs---where the model leverages examples or relevant information provided in the input context to improve predictions without parameter updates. However, while LLMs have advanced in modeling local contexts, a critical limitation remains: their restricted ability to integrate and reason over information dispersed across large codebases. This includes understanding dependencies between files, class hierarchies, and interactions with external libraries---information that is often essential for generating coherent and accurate completions.

Despite the emergence of code-specific LLMs and efforts to incorporate repository-level context during training or inference, most pre-trained models still struggle to process more than a small fraction of a project's code files at once. As a result, they fall short in capturing project-wide structure, leading to incomplete or inaccurate predictions. Therefore, there is a need for continued research within the community developing such models to address these limitations.

This thesis contributes to the field by conducting a set of comprehensive experiments and providing tools used to achieve this: a context composition framework for constructing repository context and a fine-tuning pipeline for adapting pre-trained Code Large Language Models (Code LLMs) to these compositions. The research results reveal that repository context significantly impacts completion quality during inference, and the DeepSeek-Coder-Base 1.3B model exhibits minimal effects from context-specific fine-tuning, indicating robust initial training. Furthermore, context extension maintains the in-context learning capabilities developed previously, while repository context has a marginal influence on the outcomes of context extension. In addition, notable observation is made that repository-level pre-training can achieve competitive results with significantly fewer resources, utilizing 73 million tokens compared to billions. These findings position the thesis at the intersection of code completion and project adaptation, addressing the knowledge gap in this area.

The thesis is organized as follows: \chapterref{chap:code-completion} offers an overview of the code completion task, including its definition, significance, historical approaches, and taxonomy. \chapterref{chap:standard-lm} introduces essential concepts related to Language Modeling (LM) and state-of-the-art architecture for this domain within machine learning. In \chapterref{chap:completion-centric-lm}, the discussion on language modeling is extended to encompass code completion with relevant details. \chapterref{chap:in-context-learning} covers the aspects of in-context learning, which is widely adopted in code-related tasks. The implementation of the tools used to answer the research questions is elaborated in \chapterref{chap:technical-foundation}. Finally, \chapterref{chap:research-investigation} describes the research, including precise question formulations, experimental design, and results interpretation.
