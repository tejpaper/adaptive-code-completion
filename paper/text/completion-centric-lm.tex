\chapter{Completion-Centric LM}\label{chap:completion-centric-lm}

This chapter builds upon the topic of standard language modeling by delving into the specifics of the code completion task. It provides a description of the training stages of Code LLMs, gradient masking, FIM, and the evaluation of code completion models.  

\section{Training Stages}\label{sec:training-stages}

The training of modern Code LLMs begins with acquiring a model checkpoint that possesses a robust understanding of code. This can be achieved through two distinct types of training stages. One type involves taking a general-purpose pre-trained LLM and fine-tuning it on a file-level code dataset. This approach is utilized by models such as Code Llama \parencite{rozière2023} and Qwen2.5-Coder \parencite{hui2024}. The other type involves training a model from scratch using a mixture of code and code-related natural language data. This pre-training approach is adopted by models like DeepSeek-Coder \parencite{guo2024} and OpenCoder \parencite{huang2024}. Both types necessitate vast amounts of data, typically ranging from 2 trillion (2T) to 18 trillion (18T) tokens with context length of 4,096 (4K). 

Once a strong model with a limited context size is trained, it often undergoes repository-level pre-training (sometimes referred to as long-context fine-tuning). The objective of this stage is to extend the context window, thereby enabling the model to comprehend a broader scope within a given repository. Unlike the initial training stage, repository-level pre-training requires significantly fewer tokens. However, it necessitates the application of context extrapolation methods and the utilization of longer input sequences. This phase typically involves between 8 billion (8B) and 300 billion (300B) training tokens, with the context window extended with 16K or 32K tokens.

Each input sequence in repository-level pre-training consists of two components: the composed context and the completion file. Various methods exist for obtaining the former. In this thesis, the function responsible for this task is referred to as the context composer, or simply composer. This process involves retrieving and preprocessing a subset of files, which are then assembled to form a repository context.

Models trained using only the aforementioned stages are referred to as base models. To enhance their utility for various tasks, an additional instruction tuning phase is often conducted. However, the capabilities gained from this stage are not essential for the code completion task and are not discussed further in this thesis. Throughout this work, all mentioned models are assumed to be their base versions.

\section{Gradient Masking}

The learning signal for the model is derived from both the composed context and the completion file sources. The alignment of their distributions is contingent upon the specific choice of the composer. When learning the completion of all tokens, a mismatch in these distributions can introduce undesirable bias into the model.

For instance, learning to complete README files may be irrelevant if the model's primary objective is to excel solely in code completion. However, incorporating various file formats into the context is justified if they hold relevance to the completion task (e.g., including documentation).

This issue can be mitigated through the application of gradient masking. By setting the individual losses of non-completion tokens to zero, these tokens can be excluded from the gradient computation during training. The impact of this method is examined in \sectionref{sec:gradient-masking}.

\section{Fill-in-the-Middle}\label{sec:fill-in-the-middle}

Due to the autoregressive nature of decoder-only Transformers, they are unable to utilize future tokens in their context. Consequently, to account for both the prefix and suffix of the completion file, the fill-in-the-middle approach was proposed by \citet{bavarian2022}, with its origins in the works of \citet{donahue2020}, \citet{aghajanyan2022}, and \citet{fried2022}. This capability is particularly advantageous for the completion task, as code is frequently written in a non-sequential and chaotic order.

The primary concept of FIM involves randomly dividing a portion of training sequences into three segments: prefix, middle, and suffix. These segments are then concatenated using special tokens to form a new sequence order: prefix, suffix, and middle. Incorporating such augmented data into the pre-training process, introduces a new infilling capability to the model, albeit with a marginal performance degradation (\cite{allal2023};~\cite{rozière2023}).

\section{Evaluation}

The assessment of code completion models requires a multifaceted approach that captures both syntactic and functional aspects of generated code. This section surveys the principal methodologies for evaluating model outputs, highlighting the diversity of metrics and their relevance to practical development scenarios.

\subsection{Metrics}

A variety of metrics have been developed to quantify the performance of code completion systems. These measures reflect different perspectives on correctness, similarity, and usability, ranging from exact syntactic matches to more nuanced assessments of edit effort and semantic equivalence. The following overview presents the most prominent of them.

\subsubsection*{Exact Match}

The exact match (EM) metric is a fundamental and widely used measure for evaluating code completion. It is defined as the ratio of correctly completed code lines to the total number of lines. This metric is particularly valued for its direct alignment with the objectives of code completion evaluation.

However, the exact match metric operates at a line-level granularity, which means it only indicates whether a line is completed correctly or not. This binary nature of the metric does not offer insights into the degree of deviation from the correct completion in cases where the completion is incorrect. To mitigate this limitation, the exact match metric is often used in conjunction with the more granular measures.

\subsubsection*{Edit Similarity}

The generation of functionally correct code completions is valuable, yet the effort required to edit and adapt these generated lines is equally significant. Edit similarity (ES) quantifies this effort by measuring the number of single-character edits (insertions, deletions, or substitutions) needed to transform the generated code into a reference \parencite{svyatkovskiy2020}. Mathematically, ES for two strings is expressed as:
\begin{equation}
    \text{\textsc{ES}}(\bm{x}, \bm{y}) = 1 - \frac{\mathrm{lev}(\bm{x}, \bm{y})}{\max\{|\bm{x}|, |\bm{y}|\}},
\end{equation}
where \(\mathrm{lev}(\bm{x}, \bm{y})\) represents the Levenshtein distance between the generated code \(\bm{x}\) and the reference sample \(\bm{y}\), and \(|\cdot|\) denotes the length of the string.

This metric is crucial for evaluating code completion scenarios, as it provides a measure of the effort developers must exert to correct errors in the generated code. Moreover, it has been shown that edit similarity moderately correlates with the generation of functionally correct code \parencite{dibia2022}.

\subsubsection*{Cross-Entropy and Perplexity}

As mentioned earlier, cross-entropy is employed as a loss function in the training process of language models. More specifically, it represents the average log-likelihood of the individual ground truth tokens. It can also be interpreted as the average number of nats required to encode the model's predictions per token compared to the true distribution.

Perplexity (PPL) is the exponentiated form of cross-entropy, providing a more intuitive interpretation as the average number of choices among which the model is uncertain. For this reason, perplexity is often referred to as the weighted average branching factor of a language \parencite{murphy2022}.

Both metrics are frequently used as proxies for assessing model quality. They are valuable for monitoring because these measures are consistently computed during training, being either derived from the loss function (PPL) or representing the loss itself (CE). However, they are too abstract to serve as primary metrics for specific tasks. Additionally, these measures are heavily influenced by vocabulary size and tokenization methods, which makes them unsuitable for comparing different models.

\subsubsection*{Top-\(k\) Accuracy}

Top-\(k\) accuracy is a metric that quantifies the frequency with which the model's top-\(k\) predictions align with the actual ground truth completion. Within the scope of this thesis, top-\(k\) accuracy is considered as a token-level metric to provide a more granular evaluation of the model's performance.

\subsubsection*{Pass@k}

\begin{sloppypar}
All aforementioned metrics are syntax-based, meaning they evaluate the model's performance based on the syntactic match between the generated and reference completions. However, in real-world scenarios, there exists a wide variety of correct completions that may not be present in the dataset used for evaluation. The model might predict a completion that fulfills the user's functional requirements, even if the metric evaluates it as incorrect.
\end{sloppypar}

To address this issue, an unbiased estimation of the probability that at least one generated solution out of \(k\) passes all unit tests was proposed by \citet{chen2021}. This metric, known as pass@k, is calculated using the following expectation:
\begin{equation}
    \text{pass@k} = \underset{\mathrm{Problems}}{\mathbb{E}}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right],
\end{equation}
where \(n \ge k\) is the number of samples generated by the model, and \(c \le n\) is the number of those samples that passed all unit tests. This metric reflects the probability of finding a correct solution within \(k\) attempts, which mirrors the iterative approach developers often take when exploring multiple solutions.

To apply this metric to single-line code completion, it is assumed that all other lines, except the target one, are present and functionally correct. This assumption does not hold for many use cases, such as when a user writes a function on the fly. Therefore, pass@k offers an optimistic assessment of completion capabilities, as the evaluation set contains one corrupted line per problem. In addition to this, \citet{liu2024} introduced a comprehensive list of further limitations in their Appendix A.4 section. In summary, pass@k is more realistic for tasks with greater completion granularity or for code generation assessment.

\subsubsection*{Further Metrics}

Several metrics for evaluating generated code have been migrated from machine translation and summarization fields, yet they often face limitations due to their unnatural application to code \parencite{evtikhiev2022}. This has prompted the research community to develop code-specific adaptations. However, identifying an optimal metric for code-oriented tasks remains an open challenge.

\begin{sloppypar}
\paragraph{\(N\)-gram Based Metrics:} The bilingual evaluation understudy (BLEU) \parencite{papineni2002} is a precision-oriented metric that measures the proportion of \(n\)-grams in the candidate text that also appear in the reference snippet. BLEU4 is a specific instance of BLEU that considers \(n\)-grams up to a length of 4. This configuration is widely adopted because it balances the evaluation of both individual words and longer phrases. The recall-oriented understudy for gisting evaluation (ROUGE) \parencite{lin2004} is a family of recall-based metrics originally developed for evaluating automatic summarization. ROUGE-N calculates \(n\)-gram recall between a candidate and a reference, while ROUGE-L uses the longest common subsequence to capture word order without requiring consecutive matches.
\end{sloppypar}

\begin{sloppypar}
\paragraph{Character-Level Metric:} The ChrF metric \parencite{popovic2015} is an \(F\)-measure that evaluates character \(n\)-grams instead of word ones. It incorporates a \(\beta\) parameter to adjust the emphasis between precision and recall. ChrF represents an advancement over previously mentioned metrics by balancing precision and recall in a single measure. The character-level approach offers several advantages: it eliminates tokenization dependencies, implicitly captures morpho-syntactic information, and requires no additional linguistic tools.
\end{sloppypar}

\paragraph{Semantic-Enhanced Metrics:} Metric for evaluation of translation with explicit ordering (METEOR) \parencite{banarjee2005} improves upon simple \(n\)-gram matching by adding support for stemming, synonymy, and paraphrasing, along with a penalty for disordered \(n\)-grams. BERTScore \parencite{zhang2019} leverages contextual embeddings from BERT (Bidirectional Encoder Representations from Transformers) to compute similarity between tokens in candidate and reference texts, enabling robust matching of paraphrases and semantic equivalents.

\paragraph{Code-Specific Metrics:} RUBY \parencite{tran2019} was specifically designed for code evaluation, integrating lexical, syntactic, and semantic representations of source code to better capture functional equivalence. It uses an ensemble approach that leverages the most reliable available representation: program dependence graphs, abstract syntax trees, and surface text. CodeBLEU \parencite{ren2020} extends BLEU by adding weighted \(n\)-gram matching for programming language keywords, an abstract syntax tree match component to capture structural information, and a data-flow match to evaluate semantic correctness.

\subsection{Evaluation Modes}

Besides the chosen inference strategy, types of which were discussed in \sectionref{sec:inference}, there are multiple setups for evaluating the repository-level completion capabilities of the Code LLM.

During training, language models typically employ a technique known as teacher forcing. In this method, each subsequent token prediction is conditioned on the ground truth tokens from the preceding sequence. This approach takes advantage of the parallelizable nature of Transformers, facilitating efficient training. Conversely, during evaluation, the model can be conditioned on its own previously generated tokens, offering a more realistic assessment of its capabilities. Despite this, teacher forcing remains prevalent during training for the purpose of collecting metrics and monitoring progress, as it does not require additional forward passes. It is crucial to recognize that teacher forcing can render some metrics meaningless, as they may assume conditioning on the model's output distribution. For example, edit similarity necessitates the generation of a full line solely by the model without external guidance, as it compares entire lines rather than individual tokens. The same statement does not hold for exact match, where a single incorrectly generated token results in the entire line being deemed incorrect.

For single-line code completion, the repository context can be composed based on the file prefix (and suffix in the case of FIM) or the entire file. The former method requires a separate composition and forward pass for each target line in the composition file, offering superior performance. In contrast, the latter requires only a single instance of both, but it risks data leakage of the target line for the retrieval mechanism. Despite the potential issues of the second method, it is suitable for training as it does not necessitate gradient masking for the file prefix and provides more training tokens for loss computation.

Apart from training evaluation, a validation loop is employed periodically to monitor the decline in a model's ability to generalize to new data, a phenomenon known as overfitting, which occurs due to excessive memorization of the training data. To ensure that validation metrics are comparable to training ones, the same evaluation setup is often utilized. However, this approach differs from the final evaluation of the model on various benchmarks, which is conducted using scenarios that closely resemble real-world conditions.

\subsection{Benchmarks}\label{sec:benchmarks}

In recent years, the field of code completion has witnessed significant advancements in benchmarking, particularly for repository-level tasks. This section presents a chronological overview of major benchmarks that have shaped this domain.

CodeXGLUE, introduced by \citet{lu2021}, represents one of the first comprehensive benchmarks for code intelligence. It encompasses a collection of 10 tasks across 14 datasets, including code completion, which it evaluates using datasets like PY150 \parencite{raychev2016} and GitHub Java Corpus \parencite{allamanis2013}. While CodeXGLUE established a foundation for code completion evaluation, it primarily focused on in-file completion without considering cross-file dependencies that are prevalent in real-world software development.

Addressing this limitation, \citet{ding2022} proposed CoCoMIC, which pioneered the joint modeling of in-file and cross-file context for code completion. CoCoMIC's key innovation was the introduction of a cross-file context finder (CCFINDER) that locates and retrieves relevant cross-file context. This approach significantly improved completion accuracy, particularly for API usage scenarios, highlighting the importance of repository-level understanding in code completion tasks.

Building upon these cross-file context concepts, \citet{zhang2023a} introduced RepoEval, a benchmark constructed from high-quality real-world repositories. Unlike its predecessors, RepoEval explicitly focused on three levels of code completion granularity: line, API invocation, and function body completion scenarios. The benchmark also employs unit tests present in the repositories for evaluating function body completions. RepoEval's design brought the evaluation closer to real-world development practices.

RepoBench, proposed by \citet{liu2023}, address RepoEval's limitation of limited repository coverage by significantly expanding the evaluation dataset. It further enhanced the evaluation framework by decomposing the repository-level code completion process into three interconnected tasks: RepoBench-R (retrieval), RepoBench-C (code completion with both 2K and 8K context lengths), and RepoBench-P (end-to-end pipeline). This modular approach enabled more targeted evaluations of each component while maintaining their interdependence in a complete system. RepoBench also expanded language coverage to include both Python and Java, representing a step toward multilingual evaluation. In addition, RepoBench demonstrated that incorporating cross-file contexts significantly improves code completion performance, even with randomly selected contexts.

CrossCodeEval \parencite{ding2023} significantly broadened the multilingual scope by incorporating four popular programming languages: Python, Java, TypeScript, and \Csh. It introduced a rigorous methodology for creating examples that strictly require cross-file context for accurate completion, utilizing static analysis to pinpoint cross-file context usage within the current file. Their evaluations demonstrated that models performed poorly with only in-file context but improved significantly when cross-file context was included, suggesting BM25 \parencite{robertson2009} as an effective retrieval method. This benchmark established a higher standard for evaluating cross-file contextual understanding in diverse programming languages. Later, CrossCodeLongEval \parencite{wu2024a} expanded upon this foundation by addressing CrossCodeEval's limited task coverage, introducing chunk and function completion scenarios alongside line completion to provide a more comprehensive evaluation framework.

Advancing toward more realistic scenarios, \citet{deng2024} proposed R\textsuperscript{2}C\textsuperscript{2}-Bench, which introduced a context perturbation strategy to simulate real-world repository-level code completion environments. This benchmark constructs candidate retrieval pools with both abstract and snippet contexts, capturing both coarse-grained global information and fine-grained local details. R\textsuperscript{2}C\textsuperscript{2}-Bench's comprehensive approach highlighted limitations in previous benchmarks, particularly regarding the coverage of diverse completion scenarios.

Long Code Arena (LCA), as introduced by \citet{bogomolov2024}, presented a comprehensive suite of six benchmarks, with a particular focus on evaluating repository-level capabilities across multiple code-related tasks, including project-level code completion. This component of LCA offers a more nuanced evaluation of completion tasks by categorizing target lines into six distinct types. Notably, the two most significant categories are \textit{infile} and \textit{inproject}, which respectively denote lines that utilize an API declared within the completion file and in other files within the repository. The dataset is stratified into four segments based on the total number of characters forming the overall context, thereby representing varying levels of complexity. To prevent data leakage, a traversal of Git history was conducted. In summary, Long Code Arena added an additional layer of diversity to repository-level code completion benchmarks, enhancing the evaluation landscape.

\citet{wu2024b} proposed RepoMasterEval, which prioritized testing in authentic development conditions. Recognizing limitations in test suite quality of previous benchmarks, RepoMasterEval employed mutation testing and manual test case crafting to ensure robust evaluation. Uniquely, this benchmark conducted an industrial study correlating model performance with online acceptance rates, demonstrating that RepoMasterEval scores can indeed predict real-world usability of code completion systems.

Further emphasizing developer experience, Codev-Bench \parencite{pan2024} introduced a developer-centric evaluation framework based on business analysis from industrial code completion products. It redefined evaluation criteria to better align with developers' intent and desired completion behavior throughout the coding process. Codev-Bench leveraged an agent-based system to automate repository crawling, environment construction, and dynamic call chain extraction from existing unit tests, providing a more realistic assessment of code completion in modern software development.

Most recently, \textsc{M\textsuperscript{2}rc-Eval} \parencite{liu2024} significantly expanded the multilingual coverage to 18 programming languages, addressing the limited language scope of previous benchmarks. It introduced two types of fine-grained annotations (bucket-level and semantic-level) that enable comprehensive analysis of model performance across different code semantics and complexity levels. \textsc{M\textsuperscript{2}rc-Eval} also created a massively multilingual instruction corpus to enhance repository-level code completion capabilities of existing models, establishing a new standard for comprehensive multilingual evaluation.

The evolution of these benchmarks reflects the field's progressive understanding of the complexities inherent in repository-level code completion, moving from isolated single-file evaluations to comprehensive assessments that consider cross-file dependencies, multilingual capabilities, and real-world usage patterns. These benchmarks collectively provide a robust framework for evaluating and improving code completion systems in increasingly realistic scenarios.
